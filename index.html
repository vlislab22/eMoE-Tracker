<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>ExACT</title>
    <meta name="author" content="Jiazhouzhou">
    <meta name="description" content="Project page of ExACT">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="eccv_logo.png">
    <!-- Format -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>
    <style>
        .center-image {
            display: block;
            margin-left: auto;
            margin-right: auto;
        }
    </style>
  </head>

  <body style=“text-align: center;”>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More<br /> 
                <small>
                    CVPR 2024 Highlight
                </small>
            </h1>
        </div>
        
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
			<img src="./images/jiazhou2.png" height="80px"><br>
                        <a href="https://jiazhou-garland.github.io/" >
                           Jiazhou Zhou
                        </a>
                        
                        <br /> AI Thrust, HKUST(GZ)
                        <br /> &nbsp &nbsp

                    </li>

                    <li>
			<img src="./images/xu1.png" height="80px"><br>
			<a href="https://zhengxujosh.github.io/" >
                        Xu Zheng
                      </a>
                      <br /> AI Thrust, HKUST(GZ)
                      <br /> &nbsp &nbsp
                    </li>
                <li>
                    <img src="./images/huiyi1.png" height="80px"><br>
                    <a href="https://qc-ly.github.io/" >
                                Yuanhuiyi Lyu
                                </a>
                                <br /> AI Thrust, HKUST(GZ)
                                <br /> &nbsp &nbsp
                            </li>

                            <li>
			    <img src="./images/Addision.png" height="80px"><br>
                        <a href="https://addisonwang2013.github.io/vlislab/linwang.html">
                           Addison Lin Wang
                        </a>
                        <br /> AI & CMA Thrust, HKUST(GZ) 
			    <br/> Dept. of CSE, HKUST 
                    </li>
                </ul>
            </div>
        </div>



        <!-- ##### Elements #####-->
        <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
			    <a href="">
                            <img src="./images/arxiv.png" height="100px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
			   <a href="https://youtu.be/Pz9mfuE8X1g">
                            <img src="./images/youtube_icon.jpg" height="100px"><br>
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li> 
                            <a href="https://github.com/jiazhou-garland/ExACT">
                            <img src="./images/github_icon.jpg" height="100px"><br>
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>

                        <!-- <li>
                            
                            <img src="./images/colab_icon.jpg" height="100px"><br>
                                <h4><strong>Colab</strong></h4>
                            </a>
                        </li> -->
 

                        <!-- <li>
                            <a href="https://github.com/jiazhou-garland/ELIP/blob/master/Appendix.pdf">
                            <img src="./images/slide_icon.jpg" height="100px"><br>
                                <h4><strong>Supp</strong></h4>
                            </a>
                        </li> -->
                        <li>
                            <a href="https://vlislab22.github.io/vlislab/">
                            <img src="./images/lab_logo.png" height="100px"><br>
                                <h4><strong>Vlislab</strong></h4>
                            </a>
                        </li>                       
                      
                    </ul>
                </div>
        </div>

        <!-- ##### Abstract #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
	Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns. 
        However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks.  
    	We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty.  In light of this, we propose ExACT, a novel approach that, for the first time, 
   	tackles event-based action recognition from a cross-modal conceptualizing perspective. Our ExACT brings two technical contributions. 
	Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones. 
	This subtly enhances the performance of ExACT without extra computational cost. Then, we propose a conceptual reasoning-based uncertainty estimation module, which simulates the recognition process to enrich the semantic representation. 
	In particular, conceptual reasoning builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation. 
    	Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.
                </p>
            </div>
        </div>
	    
<div class="row">
     <div class="col-md-8 col-md-offset-2">
          <h3>
              Quick Review 
          </h3>   
    </div>   
         <div class="col-md-8 col-md-offset-2">
            <video width="750"  controls >
                <source src="./video/ExACT.mp4" type="video/mp4">
              Your browser does not support HTML video.
            </video>   
    </div>          
      </div>
        <!-- ##### Results #####-->

     <div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
                The overall framework of our ExACT
            </h3>
		<p class="text-justify">
		Our ExACT consists of four components: 
		Firstly, the AFE representation recursively eliminates repeated events and generates event frames depicting dynamic actions;
		Then, the event encoder and the text encoder are responsible for the event and text embedding, respectively; 
		Lastly, the CRUE module simulates the action recognition process to establish the complex semantic relations for sub-actions and reduce the semantic uncertainty.
		</p>
            	<img src="./images/Framework.png" class="img-responsive" alt="vis_res" class="center-image" >
      	</div>
    </div>

	<div class="row">     
        <div class="col-md-8 col-md-offset-2">
            <h3>
                SeAct Dataset: Event action dataset with caption-level labels
            </h3>
		<p class="text-justify">
		We propose the semantic-abundant SeAct dataset for event-text action recognition,
where the detailed caption-level label of each action is provided. 
SeAct is collected with a DAVIS346 event camera whose resolution is 346 × 260.
It contains 58 actions under four themes, as presented in the following images. 
Each action is accompanied by an action caption of less than 30 words generated by GPT-4 to enrich the semantic space of the original action labels. 
We split 80% and 20% of each category for training and testing (validating), respectively.
		</p>
            	<img src="./images/SeACT.png"  width="600" class="center-image" >
      	</div>
    </div>


	  
   <!--##### BibTex #####-->
        <hr>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="row align-items-center">
                    <div class="col py-3">
                        <pre class="border">             
@article{ExACT,
  title={ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More},
  author={Zhou,Jiazhou, Zheng,Xu, Lyu,Yuanhuiyi and Wang,Lin},
  journal={The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024}}
		</pre>
                    </div>
                </div>
          </div>
        </div>
	    
    </div>
</body>
</html>
